1、binLog的写入机制
    * 事务执行过程中会将日志写进binlog cache，在事务提交时将binlog cache中的数据写入binlog文件中
    * 系统为binlog cache 分配一块内存，当内存空间不够时会暂存到磁盘 (内存空间大小：binlog cache size)

    事务提交时，binlog cache内容写入binlog文件
    写入磁盘的时机控制：sync_binlog
    sync_binlog => 0 每次提交事务时只写binlog文件不写磁盘
    sync_binlog => 1 每次提交事务时都写磁盘
    sync_binlog => n n个事务提交后才会写磁盘

    == 当磁盘IO是瓶颈的时候，可以适当调大sync_binlog 参数来提升性能，但是当容错率很低时，必须设置为 1
    == 通常设置为 100 - 1000 之间的值，如果异常宕机，丢失的是近N次事务的binlog

2、redoLog的写入机制
    事务执行过程中，redo log是写入到redo log buffer中，redo log的三种可能状态(redo log写入buffer和page cache 速度都很快，持久化写入磁盘就慢了很多了)
    * redo log buffer，物理上是在进程内存中
    * 写到磁盘但是未持久化，物理上实在文件系统page cache中
    * 持久化后，物理上是写在磁盘的

    redo log写入策略  innodb_flush_log_at_rx_commit
    * 0 每次事务提交redo log都写入到redo log buffer中 (只写内存，丢数据可能性较大)  后台线程每秒会将数据写入page cache中，再调用fsync写入磁盘
    * 1 每次事务提交，都持久化到磁盘  (数据不丢失，但是性能较差)
    * 2 每次事务提交，都写到page cache中  (数据库异常重启不丢失数据，主机断电会丢失，写入速度较快) 后台线程每秒写入磁盘一次

3、Mysql 备库设置为只读，不会影响主从同步吗？
    不会，主从同步数据的线程为superAdmin权限，不受readonly限制

4、一个事务日志从主库同步到从库的过程
    1) 备库执行change master， 设置主库的ip、端口、用户名、密码等信息，还包含要同步数据的位置
    2) 备库会启动两个线程，IO thread 和SQL thread，IO tread线程负责和master进行连接
    3) 主库校验完用户名密码后，根据从库发来的位置，从本地binlog开始读取数据，并发送给从库
    4) 从库接收到数据后写到本地文件中，成为中转日志
    5) 另外一个线程SQL thread 读取中转日志数据并解析执行

5、binlog日志的三种格式
    1)statement, 日志中记录原sql(风险：在主从库执行时选择索引不同，导致实际操作结果不同)
    2)row, binlog日志中记录的非sql，而是要操作的表、操作行为 和整行数据，因为记录整行数据所以可以进行恢复用  (缺点：占用空间)
    3)mixed，以上两种日志混合

    demo：delete from a limit 100000;
    * statement 格式的话就记录一条sql及一些额外信息，空间很小
    * row格式需要记录100000条日志数据....

6、MS、MM结构
    MS：client -> MasterA -> SlaveB
    MM: client -> MasterA  <--  --> MasterB // 互为主备

    双M结构的问题：互为主备，客户端的写到达A后，A将binlog同步给B，那B写完了binlog还会同步给A，循环往复
    解决：要保证两个库的server id不同，如果相同则不能互为主备。从库接受到binlog后需要将server id 与自身进行比对，如果一致则丢弃

    双M结构下日志同步的过程：
    1) 节点A更新后，生成binlog日志，日志中都是当前节点的server id
    2) binlog同步给B，节点B拿到binlog日志在本地重放后生成的binlog日志中的server id和收到的binlog中的server id保持一致
    3) A节点再收到后判断server id 相同，直接丢弃，循环结束

7、什么场景下 会将生产数据库改为非 双1策略？
    1) 备库延迟，让备库尽快赶上主库
    2) 业务高峰期

    == 通常设置为
        sync_binlog = 1000                 // 累计1000个事务写盘一次
        innodb_log_flush_as_trx_commit = 2 // 每次事务提交日志写入到page cache

8、主备延迟时间的来源
    * 主备延迟的时间主要是两部分，分为三个节点
    1) 第一个节点是主库执行完事务写入binlog 为t1
    2) 第二个节点是从库成功接收binlog并保存成功 为t2
    3) 第三个节点是从库执行完事务，为t3

    在网络正常的情况下，t1-t2的耗时非常短，主要的耗时主要集中在备库执行事务的过程中也就是t3-t2
    可能的原因:
    1) 备库机器性能较差，将多个备库部署在同一台机器上导致(这种部署时通常会将备库设置为非双1模式，增加备库数据丢失的风险，提升备库的IO吞吐)
    /** 现阶段可能随时发生主备切换，所以备库机器规格和主库一致，做对称部署 */
    2) 备库压力过高，都知道一些非核心业务不操作主库，就都放到备库上导致备库压力过高(增加从库数量)
    3) 大事务，假设主库事务执行10min，binlog传输到备库后，备库执行也要耗时10min，这10min主备是不一致的
        比如大量删除，要进行批次删除，低峰期进行；
        大表的DDL也是典型的大事务场景;
    4) 备库的并行复制能力不足导致

9、主备切换过程
    1) 判断备库的 second_behind_master 是否小于特定阈值(具体我也不知道是几), 如果小于，进行第二步，否则一直重试当前步骤
    2) 主库设置为只读，即read_only = true
    3) 判断备库的 seconds_behind_master 的值，直到为0
    4) 备库改为可读可写状态，即read_only = false
    5) 业务请求切到新主库(原备库)
    == 日常使用，建议采取可靠性优先原则

10、分布式事务的一致性
    a: 两阶段提交
    b: TCC协议 (Try-Confirm-Cancel)